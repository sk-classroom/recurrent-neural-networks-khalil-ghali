{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/sk-classroom/asc-recurrent-neural-nets/blob/main/assignments/assignment_02.ipynb)\n",
    "\n",
    "We will be creating a character-level LSTM trained on The Foundation by Isaac Asimov.  \n",
    "\n",
    "This character-level LSTM will read the book at character levels and be trained to predict the next character. By repeating the next character predictions, the LSTM will generate text in style similar to Isaac Asimov (if you train well).  \n",
    "\n",
    "# Data \n",
    "\n",
    "We will use the text in [archive.org](https://archive.org/stream/AsimovTheFoundation/Asimov_the_foundation_djvu.txt). The text is preprocessed and saved in \"data/the-foundation.txt\".  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "root = \"../\"\n",
    "\n",
    "# %% Extract the main content of the book\n",
    "## Reading and processing text\n",
    "with open(f\"{root}/data/the-foundation.txt\", \"r\", encoding=\"utf8\") as fp:\n",
    "    full_text_data = fp.read()\n",
    "\n",
    "# Remove linebreaks from the text\n",
    "full_text_data = full_text_data.replace(\"\\n\", \" \")\n",
    "full_text_data = full_text_data.replace(\"\\r\", \" \")\n",
    "full_text_data = full_text_data.replace(\"\\\\'\", \"'\")\n",
    "\n",
    "# Replace multiple spaces with a single space\n",
    "full_text_data = re.sub(r\"\\s+\", \" \", full_text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will extract the content of the book, and convert the text into a sequence of integers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Length: 1215346\n",
      "Unique Characters: 83\n"
     ]
    }
   ],
   "source": [
    "start_indx = full_text_data.find(\"THE PSYCHOHISTORIANS\")\n",
    "end_indx = len(full_text_data)\n",
    "\n",
    "full_text_data = full_text_data[start_indx:end_indx]\n",
    "char_set = set(full_text_data)\n",
    "print(\"Total Length:\", len(full_text_data))\n",
    "print(\"Unique Characters:\", len(char_set))\n",
    "\n",
    "# Tokenize the text into integers reresenting characters\n",
    "## Creating a lookup table\n",
    "chars = sorted(char_set)\n",
    "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_char = np.array(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the book is too long, we will only use the first 100000 characters for the training. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = full_text_data[:100000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, let's convert the text into an integer sequence:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_as_int = np.array(list(map(lambda x: char_to_int[x], text_data)), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: THE PSYCHOHISTORIANS \n",
      "Encoded: [45 33 30  0 41 44 50 28 33 40 33 34 44 45 40 43 34 26 39 44]\n"
     ]
    }
   ],
   "source": [
    "# The text_as_int contains the encoded values for all the characters in the text. Let's take a look at how part of our text is encoded:\n",
    "print(\"Text:\", text_data[:20], \"\\nEncoded:\", text_as_int[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dataset for training the Long-term short memory \n",
    "\n",
    "In this exercise, we will feed a fixed number of characters into the LSTM to predict the next character. While it is possible to feed all previous characters, it is not practical due to the memory and computation time. \n",
    "\n",
    "More concretely, for example, given text \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"Fox jumps over the lazy dog\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by using five previous characters, we predict the next character, namely, \n",
    "\n",
    "| Input | Target | \n",
    "|-------|--------|\n",
    "| 'F', 'o', 'x', ' ', 'j' | 'u' |\n",
    "| 'o', 'x', ' ', 'j', 'u' | 'm' |\n",
    "| 'x', ' ', 'j', 'u', 'm' | 'p' |\n",
    "| ' ', 'j', 'u', 'm', 'p' | 's' |\n",
    "| 'j', 'u', 'm', 'p', 's' | ' ' |\n",
    "| 'u', 'm', 'p', 's', ' ' | 'o' |\n",
    "| 'm', 'p', 's', ' ', 'o' | 'v' |\n",
    "\n",
    "The input and target data can be created by striding the text by a fixed number of characters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original alphabet sequence: ['F', 'o', 'x', ' ', 'j', 'u', 'm', 'p', 's', ' ', 'o', 'v', 'e', 'r', ' ', 't', 'h', 'e', ' ', 'l', 'a', 'z', 'y', ' ', 'd', 'o', 'g']\n",
      "Input: [['F', 'o', 'x', ' ', 'j'], ['o', 'x', ' ', 'j', 'u'], ['x', ' ', 'j', 'u', 'm'], [' ', 'j', 'u', 'm', 'p'], ['j', 'u', 'm', 'p', 's']]\n",
      "Target: ['u', 'm', 'p', 's', ' ']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def seq2input_target(seq, window_length):\n",
    "    input_text = [\n",
    "        list(seq[i : i + window_length]) for i in range(len(seq) - window_length)\n",
    "    ]\n",
    "    target_text = list(seq[window_length:])\n",
    "    return input_text, target_text\n",
    "\n",
    "\n",
    "chart_seq = list(example_text)\n",
    "\n",
    "print(\"Original alphabet sequence:\", chart_seq)\n",
    "\n",
    "inputs, targets = seq2input_target(chart_seq, 5)\n",
    "\n",
    "print(\"Input:\", inputs[:5])\n",
    "print(\"Target:\", targets[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is more efficient to use the integer representation of the text to create the input and target data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[31, 67, 76, 0, 62], [67, 76, 0, 62, 73], [76, 0, 62, 73, 65], [0, 62, 73, 65, 68], [62, 73, 65, 68, 71]]\n",
      "Target: [73, 65, 68, 71, 0]\n"
     ]
    }
   ],
   "source": [
    "example_text_as_int = [char_to_int[ch] for ch in example_text]\n",
    "inputs, targets = seq2input_target(example_text_as_int, 5)\n",
    "\n",
    "print(\"Input:\", inputs[:5])\n",
    "print(\"Target:\", targets[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the seq2input_target function, define a torch dataset and data loader. In this exercise, we will use the length of 30 characters for the input. Set the batch size to 32~128. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# TODO: Create a dataset for the LSTM model\n",
    "class LSTMDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, seq_data, window_length):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "\n",
    "        seq_data: The sequence data.\n",
    "            length: The length of the sequence to be used for the input and target\n",
    "        window_length: int\n",
    "        \"\"\"\n",
    "        self.window_length = window_length\n",
    "        self.seq_data = seq_data\n",
    "        self.inputs, self.targets = seq2input_target(seq_data, window_length)\n",
    "        self.inputs = torch.tensor(self.inputs, dtype=torch.long)\n",
    "        self.targets = torch.tensor(self.targets, dtype=torch.long)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]\n",
    "\n",
    "\n",
    "dataset = LSTMDataset(text_as_int, window_length=15)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=512, shuffle=False, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's define the LSTM model. \n",
    "* Define the LSTM by using torch.nn. Module\n",
    "* In addition to the components of the LSTM, add a linear layer that converts the hidden state to the output of size equal to the number of unique characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# TODO: Define the LSTM model\n",
    "class LSTM(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        # TODO: Define the gates\n",
    "        self.lin_gate_cell =torch.nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.lin_gate_input =torch.nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.lin_gate_hidden =torch.nn.Linear(input_size + hidden_size, hidden_size)\n",
    "\n",
    "        # TODO: Define the linear transformations between the cell states, inputs, and hidden states\n",
    "        self.lin_input2cell = torch.nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.lin_cell2hidden = torch.nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        # TODO: Define the linear layer that maps the hidden state to the output of size output_size\n",
    "        self.lin_hidden2output = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        # TODO: Define the activation functions, Tanh and Sigmoid\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        # TODO: Define the forward pass\n",
    "\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "\n",
    "        cell = cell * self.sigmoid(self.lin_gate_cell(combined))\n",
    "\n",
    "        cell_add = self.tanh(self.lin_input2cell(combined))\n",
    "        cell_add = cell_add * self.sigmoid(self.lin_gate_input(combined))\n",
    "        cell = cell + cell_add\n",
    "\n",
    "        next_hidden = self.tanh(self.lin_cell2hidden(cell))\n",
    "        next_hidden = next_hidden * self.sigmoid(self.lin_gate_hidden(combined))\n",
    "\n",
    "        output = self.lin_hidden2output(next_hidden)\n",
    "\n",
    "\n",
    "        return output, next_hidden, cell\n",
    "\n",
    "\n",
    "lstm = LSTM(input_size=len(char_set), hidden_size=128, output_size=len(char_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the LSTM!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "Define the loss and the optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=5e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a utility function to run the LSTM model on the sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0895,  0.0446, -0.0710, -0.0581, -0.0593, -0.0156,  0.1040, -0.0803,\n",
       "         -0.0202,  0.0478, -0.0454,  0.0406,  0.0225, -0.0114, -0.0410, -0.1146,\n",
       "          0.0271, -0.0307,  0.0555,  0.0159,  0.0488, -0.0188,  0.0382,  0.0806,\n",
       "          0.0645, -0.0414, -0.0628, -0.0402, -0.0506,  0.0611,  0.0550,  0.0314,\n",
       "          0.0905,  0.0350, -0.0812,  0.0548, -0.0160, -0.0203,  0.0217, -0.0318,\n",
       "         -0.1123,  0.0817, -0.0584, -0.0693, -0.0775, -0.0152,  0.0227,  0.0557,\n",
       "          0.0667, -0.0867, -0.0554, -0.0241, -0.0471, -0.0554,  0.0378,  0.0270,\n",
       "          0.0496,  0.0591,  0.0062, -0.0340, -0.0214, -0.0087,  0.0188,  0.0820,\n",
       "          0.0596,  0.0142,  0.0112,  0.0586, -0.0392,  0.0740, -0.0370,  0.0352,\n",
       "          0.0313, -0.0031,  0.1092, -0.1035,  0.0988, -0.0970, -0.0370,  0.0693,\n",
       "         -0.0318,  0.0055, -0.0212],\n",
       "        [ 0.0870,  0.0473, -0.0653, -0.0552, -0.0622, -0.0133,  0.1078, -0.0870,\n",
       "         -0.0291,  0.0525, -0.0389,  0.0461,  0.0245, -0.0103, -0.0438, -0.1205,\n",
       "          0.0262, -0.0318,  0.0590,  0.0169,  0.0519, -0.0190,  0.0308,  0.0829,\n",
       "          0.0681, -0.0356, -0.0658, -0.0413, -0.0509,  0.0575,  0.0529,  0.0370,\n",
       "          0.0981,  0.0292, -0.0845,  0.0538, -0.0182, -0.0145,  0.0246, -0.0284,\n",
       "         -0.1171,  0.0765, -0.0593, -0.0682, -0.0700, -0.0095,  0.0205,  0.0619,\n",
       "          0.0678, -0.0883, -0.0606, -0.0263, -0.0467, -0.0526,  0.0433,  0.0232,\n",
       "          0.0452,  0.0579,  0.0046, -0.0331, -0.0201, -0.0037,  0.0208,  0.0891,\n",
       "          0.0567,  0.0059,  0.0129,  0.0555, -0.0371,  0.0754, -0.0369,  0.0330,\n",
       "          0.0373, -0.0031,  0.1118, -0.1047,  0.1021, -0.0959, -0.0371,  0.0725,\n",
       "         -0.0319,  0.0016, -0.0235],\n",
       "        [ 0.0774,  0.0470, -0.0652, -0.0585, -0.0602, -0.0106,  0.1033, -0.0859,\n",
       "         -0.0234,  0.0452, -0.0382,  0.0466,  0.0303, -0.0159, -0.0449, -0.1243,\n",
       "          0.0319, -0.0344,  0.0616,  0.0102,  0.0530, -0.0228,  0.0344,  0.0859,\n",
       "          0.0676, -0.0325, -0.0637, -0.0411, -0.0485,  0.0629,  0.0544,  0.0332,\n",
       "          0.0969,  0.0353, -0.0816,  0.0611, -0.0147, -0.0074,  0.0206, -0.0290,\n",
       "         -0.1214,  0.0788, -0.0589, -0.0687, -0.0724, -0.0094,  0.0186,  0.0556,\n",
       "          0.0677, -0.0906, -0.0588, -0.0241, -0.0475, -0.0562,  0.0415,  0.0191,\n",
       "          0.0474,  0.0535,  0.0055, -0.0269, -0.0256, -0.0087,  0.0233,  0.0843,\n",
       "          0.0504,  0.0097,  0.0066,  0.0542, -0.0363,  0.0736, -0.0422,  0.0300,\n",
       "          0.0402,  0.0021,  0.1144, -0.1050,  0.0984, -0.0976, -0.0355,  0.0692,\n",
       "         -0.0315,  0.0011, -0.0229]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Define this function\n",
    "def run_ltsm(seqs, lstm, hidden_size):\n",
    "    \"\"\"Run the LSTM model on the sequences\n",
    "\n",
    "    Args:\n",
    "    seqs: The input sequences\n",
    "    lstm: The LSTM model\n",
    "    hidden_size: The size of the hidden states\n",
    "\n",
    "    Returns:\n",
    "    output: The output of the LSTM for the last character in the sequence\n",
    "    \"\"\"\n",
    "    n_seqs = seqs.shape[0]\n",
    "    seq_length = seqs.shape[1]\n",
    "\n",
    "    # TODO: Initialize the hidden and cell states\n",
    "    # Hint:\n",
    "    # - Use torch.zeros to initialize the hidden and cell states\n",
    "    hidden = torch.zeros(n_seqs, hidden_size, dtype=torch.float32)\n",
    "    cell = torch.zeros(n_seqs, hidden_size, dtype=torch.float32)\n",
    "\n",
    "    # TODO: Run the LSTM model on the sequences\n",
    "    # Hint:\n",
    "    # - Use torch.nn.functional.one_hot to convert the input sequences to one-hot vectors\n",
    "    # - Then, feed the one-hot vectors to the LSTM model\n",
    "    for i in range(seq_length):\n",
    "        vecs = torch.nn.functional.one_hot(seqs[:, i], num_classes=len(char_set)).float()\n",
    "        output, hidden, cell = lstm.forward(vecs, hidden, cell)\n",
    "    return output\n",
    "\n",
    "\n",
    "example_text_as_int = [char_to_int[ch] for ch in example_text]\n",
    "inputs, targets = seq2input_target(example_text_as_int, 5)\n",
    "\n",
    "run_ltsm(torch.tensor(inputs[:3]), lstm, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the LSTM model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [00:16<00:00, 12.09it/s]\n",
      "100%|██████████| 195/195 [00:17<00:00, 11.45it/s]\n",
      "100%|██████████| 195/195 [00:16<00:00, 11.55it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "n_epochs = 3\n",
    "loss_values = []\n",
    "\n",
    "# TODO: Train the LSTM model\n",
    "for epoch in range(n_epochs):\n",
    "    pbar = tqdm(dataloader)\n",
    "\n",
    "    for inputs, targets in pbar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = run_ltsm(inputs.long(), lstm, 128)\n",
    "\n",
    "        loss = criterion(output, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_values)\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the text by using the trained LSTM\n",
    "\n",
    "We will generate the text by using the trained LSTM. \n",
    "\n",
    "1. We will feed the first 30 characters into the LSTM.\n",
    "2. Use the output to predict the next character. \n",
    "3. Add the predicted character to the input and remove the first character. \n",
    "4. Repeat this process to generate the text. \n",
    "\n",
    "A word is selected randomly based on the probability of the next character calculated by the softmax function as follows:\n",
    "$$\n",
    "P(w | w_{t-1}, w_{t-2}, \\ldots, w_1) = \\frac{\\exp(\\text{LSTM}(w)/T)}{\\sum_{w'} \\exp(\\text{LSTM}(w')/T)}, \n",
    "$$\n",
    "where $T>0$ represents the temperature parameter that controls the randomness of the output. A large temperature value results in a more random output, while a small temperature value results in a more deterministic output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My nam the sall and and a don the he the ald the the the the the the serent and to the was the beat to the'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_text(\n",
    "    lstm, start_text, length, window_length, char_to_int, int_to_char, temp\n",
    "):\n",
    "    \"\"\"Generate text using the LSTM model\n",
    "\n",
    "    Args:\n",
    "    lstm: The LSTM model\n",
    "    start_text: The starting text\n",
    "    length: The length of the generated text\n",
    "    window_length: The length of the input sequence\n",
    "    char_to_int: The character to integer mapping\n",
    "    int_to_char: The integer to character mapping\n",
    "\n",
    "    Returns:\n",
    "    generated_text: The generated text\n",
    "    \"\"\"\n",
    "    start_text_as_int = [char_to_int[ch] for ch in start_text]\n",
    "    generated_text = start_text_as_int\n",
    "\n",
    "    for i in range(length):\n",
    "        input_text = generated_text[-np.minimum(window_length, len(generated_text)) :]\n",
    "        input_text = torch.tensor(input_text).view(1, -1)\n",
    "        output = run_ltsm(input_text, lstm, 128)\n",
    "        output = torch.softmax(output / temp, dim=1).multinomial(num_samples=1)\n",
    "        generated_text.append(output.item())\n",
    "\n",
    "    generated_text = [int_to_char[i] for i in generated_text]\n",
    "    return \"\".join(generated_text)\n",
    "\n",
    "\n",
    "generate_text(lstm, \"My nam\", 100, 30, char_to_int, int_to_char, temp=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated text would not be readable. Still, you might see some unique words used in the book. Such as \"Seldon\", \"Second Foundation\".\n",
    "\n",
    "Interested students are encouraged to expand the training data by changing the following line in the code at the third cell. \n",
    "\n",
    "```python \n",
    "#text_data = full_text_data[:10000] # old\n",
    "text_data = full_text_data[:1000000] # new \n",
    "#text_data = full_text_data[:10000000] # (if you have time) \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the learning is successful, the trained LTCM should be able to predict the next character better than a random prediction. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Two Men and the Elders  The Elders of this particular region of Rossem were not exactly what one might have expected. They were not a mere extrapolation of the peasantry; older, more authoritative, less friendly.  Not at all.  The dignity that had marked them at first meeting had grown in impression till it had reached the mark of being their predominant characteristic.  They sat about their oval table like so many grave and slow-moving thinkers. Most were a trifle past their physical prime, though the few who possessed beards wore them short and neatly arranged. Still, enough appeared younger than forty to make it quite obvious that \"Elders\" was a term of respect rather than entirely a literal description of age.  The two from outer space were at the head of the table and in the solemn silence that accompanied a rather frugal meal that seemed ceremonious rather than nourishing, absorbed the new, contrasting atmosphere.  After the meal and after one or two respectful remarks - too short and simple to be called speeches - had been made by those of the Elders apparently held most in esteem, an informality forced itself upon the assembly.  It was as if the dignity of greeting foreign personages had finally given way to the amiable rustic qualities of curiosity and friendliness.  They crowded around the two strangers and the flood of questions came.  They asked if it were difficult to handle a spaceship, how many men were required for the job, if better motors could be made for their ground-cars, if it was true that it rarely snowed on other worlds as was said to be the case with Tazenda, how many people lived on their world, if it was as large as Tazenda, if it was far away, how their clothes were woven and what gave them the metallic shimmer, why they did not wear furs, if they shaved every day, what sort of stone that was in Pritcher\\'s ring - The list stretched out.  And almost always the questions were addressed to Pritcher as though, as the elder, they automatically invested him with the greater authority. Pritcher found himself forced to answer at greater and greater length. It was like an immersion in a crowd of children. Their questions were those of utter and disarming wonder. Their eagerness to know was completely irresistible and would not be denied.  Pritcher explained that spaceships were not difficult to handle and that crews varied with the size, from one to many, that the motors of their ground-cars were unknown in detail to him but could doubtless be improved, that the climates of worlds varied almost infinitely, that many hundreds of millions lived on his world but that it was far smaller and more insignificant than the great empire of Tazenda, that their clothes were woven of silicone plastics in which metallic luster was artificially produced by proper orientation of the surface molecules, and that they could be artificially heated so that furs were unnecessary, that they shaved every day, that the stone in his ring was an amethyst. The list stretched out. He found himself thawing to these    naive provincials against his will.  And always as he answered there was a rapid chatter among the Elders, as though they debated the information gained. It was difficult to follow these inner discussions of theirs for they lapsed into their own accented version of the universal Galactic language that, through long separation from the currents of living speech, had become archaic.  Almost, one might say, their curt comments among themselves hovered on the edge of understanding, but just managed to elude the clutching tendrils of comprehension.  Until finally Channis interrupted to say, \"Good sirs, you must answer us for a while, for we are strangers and would be very much interested to know all we can of Tazenda.\"  And what happened then was that a great silence fell and each of the hitherto voluble Elders grew silent. Their hands, which had been moving in such rapid and delicate accompaniment to their words as though to give them greater scope and varied shades of meaning, fell suddenly limp. They stared furtively at one another, apparently quite willing each to let the other have all the floor.  Pritcher interposed quickly, \"My companion asks this in friendliness, for the fame of Tazenda fills the Galaxy and we, of course, shall inform the governor of the loyalty and love of the Elders of Rossem.\"  No sigh of relief was heard but faces brightened. An Elder stroked his beard with thumb and forefinger, straightening its slight curl with a gentle pressure, and said: \"We are faithful servants of the Lords of Tazenda.\"  Pritcher\\'s annoyance at Channis\\' bald question subsided. It was apparent, at least, that the age that he had felt creeping over him of late had not yet deprived him of his own capacity for making smooth the blunders of others.  He continued: \"We do not know, in our far part of the universe, much of the past history of the Lords of Tazenda. We presume they have ruled benevolently here for a long time.\"  The same Elder who spoke before, answered. In a soft, automatic way he had become spokesman. He said: \"Not the grandfather of the oldest can recall a time in which the Lords were absent.\"  \"It has been a time of peace?\"  \"It has been a time of peace!\" He hesitated. \"The governor is a strong and powerful Lord who would not hesitate to punish traitors. None of us are traitors, of course.\"  \"He has punished some in the past, I imagine, as they deserve.\"  Again hesitation, \"None here have ever been traitors, or our fathers or our fathers\\' fathers. But on other worlds, there have been such, and death followed for them quickly. It is not good to think of for we are humble men who are poor farmers and not concerned with matters of politics.\"  The anxiety in his voice, the universal concern in the eyes of all of them was obvious.    Pritcher said smoothly: \"Could you inform us as to how we can arrange an audience with your governor.\"  And instantly an element of sudden bewilderment entered the situation.  For after a long moment, the elder said: \"Why, did you not know? The governor will be here tomorrow. He has expected you. It has been a great honor for us. We ... we hope earnestly that you will report to him satisfactorily as to our loyalty to him.\"  Pritcher\\'s smile scarcely twitched. \"Expected us?\"  The Elder looked wonderingly from one to the other. \"Why ... it is now a week since we have been waiting for you.\"  Their quarters were undoubtedly luxurious for the world. Pritcher had lived in worse. Channis showed nothing but indifference to externals.  But there was an element of tension between them of a different nature than hitherto. Pritcher, felt the time approaching for a definite decision and yet there was still the desirability of additional waiting. To see the governor first would be to increase the gamble to dangerous dimensions and yet to win that gamble might multi-double the winnings. He felt a surge of anger at the slight crease between Channis\\' eyebrows, the delicate uncertainty with which the young man\\'s lower lip presented itself to an upper tooth. He detested the useless play-acting and yearned for an end to it.  He said: \"We seem to be anticipated.\"  \\'Yes,\" said Channis, simply.  \"Just that? You have no contribution of greater pith to make. We come here and find that the governor expects us. Presumably we shall find from the governor that Tazenda itself expects us. Of what value then is our entire mission?\"  Channis looked up, without endeavoring to conceal the weary note in his voice: \"To expect us is one thing; to know who we are and what we came for, is another.\"  \"Do you expect to conceal these things from men of the Second Foundation?\"  \"Perhaps. Why not? Are you ready to throw your hand in? Suppose our ship was detected in space. Is it unusual for a realm to maintain frontier observation posts? Even if we were ordinary strangers, we would be of interest.\"  \"Sufficient interest for a governor to come to us rather than the reverse?\\'  Channis shrugged: \"We’ll have to meet that problem later. Let us see what this governor is like.\"  Pritcher bared his teeth in a bloodless kind of scowl. The situation was becoming ridiculous.  Channis proceeded with an artificial animation: \"At least we know one thing. Tazenda is the Second Foundation or a million shreds of evidence are unanimously pointing the wrong way.    How do you interpret the obvious terror in which these natives hold Tazenda? I see no signs of political domination. Their groups of Elders apparently meet freely and without interference of any sort. The taxation they speak of doesn\\'t seem at all extensive to me or efficiently carried through. The natives speak much of poverty but seem sturdy and well-fed. The houses are uncouth and their villages rude, but are obviously adequate for the purpose.  \"In fact, the world fascinates me. I have never seen a more forbidding one, yet I am convinced there is no suffering among the population and that their uncomplicated lives manage to contain a well-balanced happiness lacking in the sophisticated populations of the advanced centers.\"  \"Are you an admirer of peasant virtues, then?\"  \"The stars forbid.\" Channis seemed amused at the idea. \"I merely point out the significance of all this. Apparently, Tazenda is an efficient administrator - efficient in a sense far different from the efficiency of the old Empire or of the First Foundation, or even of our own Union. All these have brought mechanical efficiency to their subjects at the cost of more intangible values. Tazenda brings happiness and sufficiency. Don\\'t you see that the whole orientation of their domination is different? It is not physical, but psychological.\"  \"Really?\" Pritcher, allowed himself irony. \"And the terror with which the Elders spoke of the punishment of treason by these kind hearted psychologist administrators? How does that suit your thesis?\"  \"Were they the objects of the punishment? They speak of punishment only of others. It is as if knowledge of punishment has been so well implanted in them that punishment itself need never be used. The proper mental attitudes are so inserted into their minds that I am certain that not a Tazendian soldier exists on the planet. Don\\'t you see all this?\"  \"I’ll see perhaps,\" said Pritcher, coldly, \"when I see the governor. And what, by the way, if our mentalities are handled?\"  Channis replied with brutal contempt: \"You should be accustomed to that.\"  Pritcher whitened perceptibly, and, with an effort, turned away. They spoke to one another no more that day.  It was in the silent windlessness of the frigid night, as he listened to the soft, sleeping motions of the other, that Pritcher silently adjusted his wrist-transmitter to the ultrawave region for which Channis\\' was unadjustable and, with noiseless touches of his fingernail, contacted the ship.  The answer came in little periods of noiseless vibration that barely lifted themselves above the sensory threshold.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(f\"{root}/data/the-foundation-test.txt\", \"r\", encoding=\"utf8\") as fp:\n",
    "    eval_text = fp.read()\n",
    "\n",
    "eval_text = eval_text.replace(\"\\n\", \" \")\n",
    "eval_text = eval_text.replace(\"\\r\", \" \")\n",
    "eval_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3651\n",
      "Random Accuracy: 0.0120\n"
     ]
    }
   ],
   "source": [
    "eval_text_as_int = [char_to_int[ch] for ch in eval_text]\n",
    "inputs, targets = seq2input_target(eval_text_as_int, window_length=30)\n",
    "\n",
    "output = run_ltsm(torch.tensor(inputs), lstm, 128)\n",
    "predictions = torch.argmax(output, dim=1).view(-1).to(\"cpu\").numpy()\n",
    "\n",
    "# To char\n",
    "predictions = np.array([int_to_char[i] for i in predictions])\n",
    "targets = np.array([int_to_char[i] for i in targets])\n",
    "\n",
    "acc = np.mean(predictions == np.array(targets))\n",
    "rand_acc = 1.0 / len(char_set)\n",
    "\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Random Accuracy: {rand_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit the results\n",
    "\n",
    "Please **git commit & push** the following two files created in the following cell. \n",
    "\n",
    "1. \"~/assignments/lstm_test_predictions.txt\" \n",
    "\n",
    "2. \"~/assignments/lstm_loss_values.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./lstm_test_predictions.txt\", \"w\", encoding=\"utf8\") as f:\n",
    "    for char in predictions:\n",
    "        f.write(char)\n",
    "\n",
    "pd.DataFrame(loss_values).to_csv(\"./lstm_loss_values.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "applsoftcomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
